{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910b3620",
   "metadata": {},
   "source": [
    "# Variant Calling Analysis (Improved)\n",
    "Based on [Circulating tumor DNA sequencing in colorectal cancer patients treated with first-line chemotherapy with anti-EGFR](https://www.nature.com/articles/s41598-021-95345-4).\n",
    "___\n",
    "\n",
    "Analysis by: Luis Aguilera. December 2, 2025\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63377509",
   "metadata": {},
   "source": [
    "## Project Description:\n",
    "This pipeline analyzes circulating tumor DNA sequencing data to track KRAS mutations in colorectal cancer patients during anti-EGFR treatment.\n",
    "This code aims to identify variant allele frequency changes that serve as biomarkers for monitoring treatment response and tumor evolution.\n",
    "\n",
    "**Improvements in this version:**\n",
    "- **Quality Control**: FastQC for raw read assessment.\n",
    "- **Trimming**: fastp for adapter and quality trimming.\n",
    "- **Somatic Calling**: Lofreq for high-sensitivity detection of low-frequency variants (ctDNA).\n",
    "- **Annotation**: SnpEff for functional annotation of variants.\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40fdaab",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6553e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pysradb import SRAweb\n",
    "import requests\n",
    "import pysam\n",
    "import matplotlib.pyplot as plt\n",
    "from dna_features_viewer import GraphicFeature, GraphicRecord\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308883e",
   "metadata": {},
   "source": [
    "### Paths to data\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current working directory\n",
    "cwd = Path.cwd()\n",
    "print(\"Current working directory:\", cwd)\n",
    "# Data directory\n",
    "data_dir = cwd / \"data\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Genome reference directory\n",
    "genome_ref_dir = data_dir / \"genome_reference\"\n",
    "genome_ref_dir.mkdir(parents=True, exist_ok=True)\n",
    "# QC directory\n",
    "qc_dir = data_dir / \"qc\"\n",
    "qc_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e67c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to run shell commands\n",
    "def run_cmd(cmd):\n",
    "    subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6aaf90",
   "metadata": {},
   "source": [
    "### Downloading the reference genome (GRCh38)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10744a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_reference_genome(fasta_url, genome_ref_directory):\n",
    "    \"\"\"\n",
    "    Download and decompress human reference genome.\n",
    "    Input: fasta_url (str), genome_ref_directory (Path)\n",
    "    Output: Path to decompressed FASTA file\n",
    "    \"\"\"\n",
    "    # get the name from the URL\n",
    "    gz_filename = Path(fasta_url).name\n",
    "    fa_filename = gz_filename.replace(\".gz\", \"\")\n",
    "    gz_path = genome_ref_directory / gz_filename # Path to compressed file\n",
    "    fa_path = genome_ref_directory / fa_filename # Path to decompressed FASTA file\n",
    "    if not fa_path.exists():\n",
    "        if not gz_path.exists():\n",
    "            # Download the gzipped fasta file\n",
    "            run_cmd([\"wget\", \"-O\", str(gz_path), fasta_url])\n",
    "        # Decompress the fasta file\n",
    "        run_cmd([\"gunzip\", \"-k\", str(gz_path)])\n",
    "    else:\n",
    "        print(\"Reference genome already exists.\")\n",
    "    return fa_path\n",
    "\n",
    "# Download reference genome\n",
    "human_genome_fasta_url = (\"https://ftp.ensembl.org/pub/release-109/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\")\n",
    "fa_ref_genome_path = download_reference_genome(human_genome_fasta_url, genome_ref_dir)\n",
    "fa_ref_genome_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c2782",
   "metadata": {},
   "source": [
    "### Get metadata for bioproject: PRJNA714799\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SRA metadata for project PRJNA714799\n",
    "metadata_file_path = data_dir.joinpath(\"SRA_metadata_PRJNA714799_full.csv\")\n",
    "if metadata_file_path.exists():\n",
    "    metadata = pd.read_csv(metadata_file_path)\n",
    "else:\n",
    "    db = SRAweb()\n",
    "    metadata = db.sra_metadata(\"PRJNA714799\", detailed=True)\n",
    "    metadata.to_csv(metadata_file_path, index=False)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sample type and ID from 'tissue' column\n",
    "metadata['sample_type'] = metadata['tissue'].str.extract(r'^([A-Za-z]+)_')[0]\n",
    "metadata['sample_id'] = metadata['tissue'].str.extract(r'_(\\d+)$')[0].astype(int)\n",
    "print(f\"{metadata['sample_type'].value_counts().to_string()}\\n\")\n",
    "print(f\"Unique patient IDs: {metadata['sample_id'].nunique()}\\n\")\n",
    "print(f\"{metadata['isolate'].value_counts().to_string()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa9091",
   "metadata": {},
   "source": [
    "### Extracting patient_id and timepoints from metadata\n",
    "___\n",
    "\n",
    "The SRA metadata use a consistent `run_alias` pattern:\n",
    "\n",
    "- PBMC samples: `PBMC_<ID>.final.bam`  \n",
    "  patient_id = <ID>, sample_type = PBMC, no timepoint\n",
    "\n",
    "- FFPE tumor samples: `FFPE_<ID>.final.bam`  \n",
    "  patient_id = <ID>, sample_type = Tissue, no timepoint\n",
    "\n",
    "- ctDNA samples: `<ID>.final.bam` or `<ID>-<N>.final.bam`  \n",
    "  patient_id = <ID>, sample_type = ctDNA, timepoint 0 (no suffix) or timepoint N (suffix -N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patient_info_from_alias(run_alias):\n",
    "    \"\"\"\n",
    "    Parse run_alias to extract patient_id, timepoint, and sample_type.\n",
    "    Input: run_alias (str)\n",
    "    Output: dict with keys patient_id, timepoint, sample_type\n",
    "    \"\"\"\n",
    "    if pd.isna(run_alias):\n",
    "        return {\"patient_id\": None, \"timepoint\": None, \"sample_type\": None}\n",
    "    if run_alias.startswith(\"PBMC_\"):\n",
    "        patient_id = run_alias.replace(\"PBMC_\", \"\").replace(\".final.bam\", \"\")\n",
    "        return {\"patient_id\": patient_id, \"timepoint\": None, \"sample_type\": \"PBMC\"}\n",
    "    if run_alias.startswith(\"FFPE_\"):\n",
    "        patient_id = run_alias.replace(\"FFPE_\", \"\").replace(\".final.bam\", \"\")\n",
    "        return {\"patient_id\": patient_id, \"timepoint\": None, \"sample_type\": \"Tissue\"}\n",
    "    ctdna_pattern = re.match(r'^(CTC\\d+|C_fw\\d+)(?:-(\\d+))?\\.final\\.bam$', run_alias)\n",
    "    if ctdna_pattern:\n",
    "        patient_id = ctdna_pattern.group(1)\n",
    "        timepoint = 0 if ctdna_pattern.group(2) is None else int(ctdna_pattern.group(2))\n",
    "        return {\"patient_id\": patient_id, \"timepoint\": timepoint, \"sample_type\": \"ctDNA\"}\n",
    "    return {\"patient_id\": None, \"timepoint\": None, \"sample_type\": None}\n",
    "\n",
    "def add_patient_info_to_metadata(input_metadata):\n",
    "    \"\"\"\n",
    "    Add patient_id, timepoint, and sample_type columns to metadata.\n",
    "    Input: input_metadata (pd.DataFrame)\n",
    "    Output: pd.DataFrame with added patient information columns\n",
    "    \"\"\"\n",
    "    output_metadata = input_metadata.copy()\n",
    "    parsed_info = output_metadata['run_alias'].apply(extract_patient_info_from_alias)\n",
    "    parsed_df = pd.DataFrame(parsed_info.tolist())\n",
    "    columns_to_drop = [\"patient_id\", \"timepoint\", \"sample_type\"]\n",
    "    existing_columns = [col for col in columns_to_drop if col in output_metadata.columns]\n",
    "    if existing_columns:\n",
    "        output_metadata = output_metadata.drop(columns=existing_columns)\n",
    "    output_metadata = pd.concat([output_metadata, parsed_df], axis=1)\n",
    "    return output_metadata\n",
    "# Parse metadata and add patient information\n",
    "metadata = add_patient_info_to_metadata(metadata)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ctdna_samples(metadata_dataframe):\n",
    "    \"\"\"\n",
    "    Filter metadata to only ctDNA samples.\n",
    "    Input: metadata_dataframe (pd.DataFrame)\n",
    "    Output: pd.DataFrame with ctDNA samples and integer timepoints\n",
    "    \"\"\"\n",
    "    ctdna_samples = metadata_dataframe[metadata_dataframe[\"sample_type\"] == \"ctDNA\"].copy()\n",
    "    ctdna_samples[\"timepoint\"] = ctdna_samples[\"timepoint\"].astype(int)\n",
    "    return ctdna_samples\n",
    "# Filter to ctDNA samples only\n",
    "ctdna = filter_ctdna_samples(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eligible_patients(ctdna_dataframe):\n",
    "    \"\"\"\n",
    "    Find patients with at least 3 timepoints including baseline (0).\n",
    "    Input: ctdna_dataframe (pd.DataFrame)\n",
    "    Output: pd.DataFrame with eligible patients and their timepoints\n",
    "    \"\"\"\n",
    "    eligible_patients_list = []\n",
    "    for patient_id, patient_group in ctdna_dataframe.groupby(\"patient_id\"):\n",
    "        timepoints = sorted(patient_group[\"timepoint\"].dropna().unique())\n",
    "        timepoints = [int(tp) for tp in timepoints]\n",
    "        has_baseline = 0 in timepoints\n",
    "        has_followup = any(tp > 0 for tp in timepoints)\n",
    "        has_three_timepoints = len(timepoints) >= 3\n",
    "        if has_baseline and has_followup and has_three_timepoints:\n",
    "            eligible_patients_list.append((patient_id, timepoints))\n",
    "    \n",
    "    eligible_df = pd.DataFrame(eligible_patients_list, columns=[\"patient_id\", \"timepoints\"])\n",
    "    eligible_df[\"n_timepoints\"] = eligible_df[\"timepoints\"].apply(len)\n",
    "    return eligible_df\n",
    "# Find eligible patients\n",
    "eligible_df = find_eligible_patients(ctdna)\n",
    "eligible_df.sort_values(\"n_timepoints\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dbbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_clinical_timepoints(patient_dataframe):\n",
    "    \"\"\"\n",
    "    Annotate timepoints with clinical phase labels.\n",
    "    Input: patient_dataframe (pd.DataFrame)\n",
    "    Output: pd.DataFrame with added tp_label column\n",
    "    \"\"\"\n",
    "    output_df = patient_dataframe.copy()\n",
    "    output_df[\"timepoint\"] = output_df[\"timepoint\"].astype(int)\n",
    "    unique_timepoints = sorted(output_df[\"timepoint\"].unique())\n",
    "    baseline_timepoint = 0\n",
    "    first_followup_timepoint = min(tp for tp in unique_timepoints if tp > 0)\n",
    "    last_timepoint = max(tp for tp in unique_timepoints if tp > 0)\n",
    "    def get_clinical_label(timepoint_value):\n",
    "        if timepoint_value == baseline_timepoint:\n",
    "            return \"pre_treatment\"\n",
    "        elif timepoint_value == first_followup_timepoint:\n",
    "            return \"during_treatment\"\n",
    "        elif timepoint_value == last_timepoint:\n",
    "            return \"post_treatment\"\n",
    "        return None\n",
    "    output_df[\"tp_label\"] = output_df[\"timepoint\"].apply(get_clinical_label)\n",
    "    return output_df.sort_values(\"timepoint\")\n",
    "# Select and annotate one patient\n",
    "selected_patient_id = \"CTC030\"\n",
    "patient_data = ctdna[ctdna[\"patient_id\"] == selected_patient_id]\n",
    "patient_data_annotated = annotate_clinical_timepoints(patient_data)\n",
    "# Display results\n",
    "patient_data_annotated[[\"run_accession\", \"run_alias\", \"timepoint\", \"tp_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b170da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_single_fastq_file(download_url, output_file_path):\n",
    "    \"\"\"\n",
    "    Download a FASTQ file if it doesn't already exist.\n",
    "    Input: download_url (str), output_file_path (Path)\n",
    "    Output: bool (True if downloaded, False if skipped)\n",
    "    \"\"\"\n",
    "    if output_file_path.exists():\n",
    "        print(f\"Skipping {output_file_path.name} (already exists)\")\n",
    "        return False\n",
    "    print(f\"Downloading {output_file_path.name}...\")\n",
    "    run_cmd([\"wget\", \"-O\", str(output_file_path), download_url])\n",
    "    return True\n",
    "\n",
    "def download_fastq_files_for_patient(patient_annotated_dataframe, patient_identifier, \n",
    "                                      data_directory, timepoints_to_include=None):\n",
    "    \"\"\"\n",
    "    Download paired FASTQ files (R1 and R2) for a patient.\n",
    "    Input: patient_annotated_dataframe (pd.DataFrame), patient_identifier (str), \n",
    "           data_directory (Path), timepoints_to_include (list, optional)\n",
    "    Output: Path (directory containing downloaded files)\n",
    "    \"\"\"\n",
    "    if timepoints_to_include is None:\n",
    "        timepoints_to_include = [\"pre_treatment\", \"during_treatment\", \"post_treatment\"]\n",
    "    patient_directory = data_directory / patient_identifier\n",
    "    patient_directory.mkdir(parents=True, exist_ok=True)\n",
    "    for _, sample_row in patient_annotated_dataframe.iterrows():\n",
    "        timepoint_label = sample_row[\"tp_label\"]\n",
    "        if timepoint_label not in timepoints_to_include:\n",
    "            continue\n",
    "        r1_url = sample_row[\"ena_fastq_http_1\"]\n",
    "        r2_url = sample_row[\"ena_fastq_http_2\"]\n",
    "        if pd.notna(r1_url) and r1_url != \"\":\n",
    "            r1_file_path = patient_directory / f\"{patient_identifier}_{timepoint_label}_R1.fastq.gz\"\n",
    "            download_single_fastq_file(r1_url, r1_file_path)\n",
    "        if pd.notna(r2_url) and r2_url != \"\":\n",
    "            r2_file_path = patient_directory / f\"{patient_identifier}_{timepoint_label}_R2.fastq.gz\"\n",
    "            download_single_fastq_file(r2_url, r2_file_path)\n",
    "    return patient_directory\n",
    "\n",
    "# Download FASTQ files for the selected patient\n",
    "patient_data_directory = download_fastq_files_for_patient(\n",
    "    patient_annotated_dataframe=patient_data_annotated,\n",
    "    patient_identifier=selected_patient_id,\n",
    "    data_directory=data_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b16469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_fastq_files_by_timepoint(patient_directory, patient_identifier):\n",
    "    \"\"\"\n",
    "    Organize FASTQ files into nested dictionary by timepoint.\n",
    "    Input: patient_directory (Path), patient_identifier (str)\n",
    "    Output: dict with structure {timepoint: {'R1': Path, 'R2': Path}}\n",
    "    \"\"\"\n",
    "    organized_fastq_paths = {}\n",
    "    filename_pattern = re.compile(\n",
    "        rf\"^{re.escape(patient_identifier)}_(.+)_R([12])\\.fastq\\.gz$\"\n",
    "    )\n",
    "    for file_path in patient_directory.iterdir():\n",
    "        if not file_path.name.endswith(\".fastq.gz\"):\n",
    "            continue\n",
    "        match = filename_pattern.match(file_path.name)\n",
    "        if match is None:\n",
    "            continue\n",
    "        timepoint_label = match.group(1)\n",
    "        read_number = match.group(2)\n",
    "        if timepoint_label not in organized_fastq_paths:\n",
    "            organized_fastq_paths[timepoint_label] = {}\n",
    "        organized_fastq_paths[timepoint_label][f\"R{read_number}\"] = file_path\n",
    "    return organized_fastq_paths\n",
    "\n",
    "# Organize FASTQ files for alignment\n",
    "fastq_paths = organize_fastq_files_by_timepoint(\n",
    "    patient_directory=patient_data_directory,\n",
    "    patient_identifier=selected_patient_id\n",
    ")\n",
    "fastq_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75eb57",
   "metadata": {},
   "source": [
    "### Quality Control and Trimming\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fastqc(fastq_files, qc_directory):\n",
    "    \"\"\"\n",
    "    Run FastQC on a list of FASTQ files.\n",
    "    Input: fastq_files (list of Path), qc_directory (Path)\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    qc_directory.mkdir(parents=True, exist_ok=True)\n",
    "    command = [\"fastqc\", \"-t\", \"4\", \"-o\", str(qc_directory)] + [str(f) for f in fastq_files]\n",
    "    print(\"Running FastQC...\")\n",
    "    run_cmd(command)\n",
    "\n",
    "def trim_reads_with_fastp(r1_path, r2_path, output_r1, output_r2, qc_directory, sample_name):\n",
    "    \"\"\"\n",
    "    Trim adapters and low-quality bases using fastp.\n",
    "    Input: r1_path, r2_path, output_r1, output_r2 (Path), qc_directory (Path), sample_name (str)\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    if output_r1.exists() and output_r2.exists():\n",
    "        print(f\"Skipping trimming for {sample_name} (already trimmed)\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Trimming {sample_name} with fastp...\")\n",
    "    html_report = qc_directory / f\"{sample_name}_fastp.html\"\n",
    "    json_report = qc_directory / f\"{sample_name}_fastp.json\"\n",
    "    \n",
    "    command = [\n",
    "        \"fastp\", \n",
    "        \"-i\", str(r1_path), \"-I\", str(r2_path),\n",
    "        \"-o\", str(output_r1), \"-O\", str(output_r2),\n",
    "        \"-h\", str(html_report), \"-j\", str(json_report),\n",
    "        \"--detect_adapter_for_pe\"\n",
    "    ]\n",
    "    run_cmd(command)\n",
    "\n",
    "# Run QC and Trimming\n",
    "patient_qc_dir = qc_dir / selected_patient_id\n",
    "trimmed_fastq_paths = {}\n",
    "\n",
    "for timepoint_label, files in fastq_paths.items():\n",
    "    # Run FastQC on raw data\n",
    "    run_fastqc([files['R1'], files['R2']], patient_qc_dir)\n",
    "    \n",
    "    # Trim reads\n",
    "    trimmed_r1 = files['R1'].with_name(files['R1'].name.replace(\".fastq.gz\", \".trimmed.fastq.gz\"))\n",
    "    trimmed_r2 = files['R2'].with_name(files['R2'].name.replace(\".fastq.gz\", \".trimmed.fastq.gz\"))\n",
    "    \n",
    "    trim_reads_with_fastp(\n",
    "        files['R1'], files['R2'], \n",
    "        trimmed_r1, trimmed_r2, \n",
    "        patient_qc_dir, f\"{selected_patient_id}_{timepoint_label}\"\n",
    "    )\n",
    "    \n",
    "    trimmed_fastq_paths[timepoint_label] = {'R1': trimmed_r1, 'R2': trimmed_r2}\n",
    "    \n",
    "trimmed_fastq_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f844a",
   "metadata": {},
   "source": [
    "### Perform alignment\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cce5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bwa_index(reference_fasta_path):\n",
    "    \"\"\"\n",
    "    Build BWA index for reference genome.\n",
    "    Input: reference_fasta_path (Path or str)\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    reference_fasta_path = Path(reference_fasta_path)\n",
    "    index_extensions = [\".amb\", \".ann\", \".bwt\", \".pac\", \".sa\"]\n",
    "    index_file_paths = [\n",
    "        reference_fasta_path.with_suffix(reference_fasta_path.suffix + ext)\n",
    "        for ext in index_extensions\n",
    "    ]\n",
    "    if all(index_file.exists() for index_file in index_file_paths):\n",
    "        print(\"BWA index already exists.\")\n",
    "        return\n",
    "    print(\"Building BWA index...\")\n",
    "    run_cmd([\"bwa\", \"index\", str(reference_fasta_path)])\n",
    "\n",
    "def align_reads_to_reference(sample_label, forward_reads_path, reverse_reads_path,\n",
    "                              reference_fasta_path, output_directory, patient_identifier):\n",
    "    \"\"\"\n",
    "    Align paired-end reads to reference genome using BWA-MEM.\n",
    "    Input: sample_label (str), forward_reads_path (Path), reverse_reads_path (Path),\n",
    "           reference_fasta_path (Path), output_directory (Path), patient_identifier (str)\n",
    "    Output: Path to sorted BAM file\n",
    "    \"\"\"\n",
    "    output_directory = Path(output_directory)\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "    sorted_bam_path = output_directory / f\"{patient_identifier}_{sample_label}.sorted.bam\"\n",
    "    bam_index_path = sorted_bam_path.with_suffix(\".bam.bai\")\n",
    "    if sorted_bam_path.exists() and bam_index_path.exists():\n",
    "        print(f\"Skipping {sorted_bam_path.name} (already aligned)\")\n",
    "        return sorted_bam_path\n",
    "    print(f\"Aligning {sample_label}...\")\n",
    "    bwa_mem_command = [\n",
    "        \"bwa\", \"mem\", \"-t\", \"10\",\n",
    "        str(reference_fasta_path),\n",
    "        str(forward_reads_path),\n",
    "        str(reverse_reads_path)\n",
    "    ]\n",
    "    process_bwa = subprocess.Popen(bwa_mem_command, stdout=subprocess.PIPE)\n",
    "    process_view = subprocess.Popen(\n",
    "        [\"samtools\", \"view\", \"-b\", \"-@\", \"4\"],\n",
    "        stdin=process_bwa.stdout,\n",
    "        stdout=subprocess.PIPE\n",
    "    )\n",
    "    process_sort = subprocess.Popen(\n",
    "        [\"samtools\", \"sort\", \"-@\", \"4\", \"-o\", str(sorted_bam_path)],\n",
    "        stdin=process_view.stdout\n",
    "    )\n",
    "    process_sort.communicate()\n",
    "    process_bwa.stdout.close()\n",
    "    process_view.stdout.close()\n",
    "    run_cmd([\"samtools\", \"index\", \"-@\", \"4\", str(sorted_bam_path)])\n",
    "    return sorted_bam_path\n",
    "\n",
    "# Build BWA index for reference genome\n",
    "build_bwa_index(fa_ref_genome_path)\n",
    "# Align all samples using TRIMMED reads\n",
    "bam_output_directory = data_dir / \"bam\" / selected_patient_id\n",
    "aligned_bam_paths = {}\n",
    "for timepoint_label, fastq_file_paths in trimmed_fastq_paths.items():\n",
    "    aligned_bam_paths[timepoint_label] = align_reads_to_reference(\n",
    "        sample_label=timepoint_label,\n",
    "        forward_reads_path=fastq_file_paths[\"R1\"],\n",
    "        reverse_reads_path=fastq_file_paths[\"R2\"],\n",
    "        reference_fasta_path=fa_ref_genome_path,\n",
    "        output_directory=bam_output_directory,\n",
    "        patient_identifier=selected_patient_id\n",
    "    )\n",
    "aligned_bam_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136052b3",
   "metadata": {},
   "source": [
    "### Duplicate removal\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_duplicate_reads(input_bam_path, output_bam_path):\n",
    "    \"\"\"\n",
    "    Mark duplicate reads in BAM file using samtools.\n",
    "    Input: input_bam_path (Path), output_bam_path (Path)\n",
    "    Output: Path to deduplicated BAM file\n",
    "    \"\"\"\n",
    "    output_bam_path = Path(output_bam_path)\n",
    "    output_bam_index = output_bam_path.with_suffix(\".bam.bai\")\n",
    "    if output_bam_path.exists() and output_bam_index.exists():\n",
    "        print(f\"Skipping {output_bam_path.name} (already marked)\")\n",
    "        return output_bam_path\n",
    "    print(f\"Marking duplicates for {input_bam_path.name}...\")\n",
    "    name_sorted_bam = str(input_bam_path).replace(\".bam\", \".name_sorted.bam\")\n",
    "    fixmate_bam = str(input_bam_path).replace(\".bam\", \".fixmate.bam\")\n",
    "    run_cmd([\"samtools\", \"sort\", \"-n\", \"-@\", \"4\", \"-o\", name_sorted_bam, str(input_bam_path)])\n",
    "    run_cmd([\"samtools\", \"fixmate\", \"-m\", \"-@\", \"4\", name_sorted_bam, fixmate_bam])\n",
    "    run_cmd([\"samtools\", \"sort\", \"-@\", \"4\", \"-o\", str(output_bam_path), fixmate_bam])\n",
    "    run_cmd([\"samtools\", \"markdup\", \"-@\", \"4\", str(output_bam_path), str(output_bam_path) + \".tmp\"])\n",
    "    run_cmd([\"mv\", str(output_bam_path) + \".tmp\", str(output_bam_path)])\n",
    "    run_cmd([\"samtools\", \"index\", \"-@\", \"4\", str(output_bam_path)])\n",
    "    Path(name_sorted_bam).unlink(missing_ok=True)\n",
    "    Path(fixmate_bam).unlink(missing_ok=True)\n",
    "    return output_bam_path\n",
    "\n",
    "deduplicated_bam_paths = {}\n",
    "for timepoint_label, bam_path in aligned_bam_paths.items():\n",
    "    output_bam = bam_path.with_name(bam_path.stem + \".markdup.bam\")\n",
    "    deduplicated_bam_paths[timepoint_label] = mark_duplicate_reads(bam_path, output_bam)\n",
    "deduplicated_bam_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6bd6f",
   "metadata": {},
   "source": [
    "### Gene Selection\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f1b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_coordinates_from_ensembl(gene_name):\n",
    "    \"\"\"\n",
    "    Retrieve genomic coordinates for a gene from Ensembl REST API.\n",
    "    Input: gene_name (str)\n",
    "    Output: str (genomic region in format chromosome:start-end)\n",
    "    \"\"\"\n",
    "    ensembl_api_url = f\"https://rest.ensembl.org/lookup/symbol/homo_sapiens/{gene_name}\"\n",
    "    api_response = requests.get(ensembl_api_url, headers={\"Content-Type\": \"application/json\"})\n",
    "    if not api_response.ok:\n",
    "        raise ValueError(f\"Gene '{gene_name}' not found\")\n",
    "    gene_data = api_response.json()\n",
    "    genomic_region = f\"{gene_data['seq_region_name']}:{gene_data['start']}-{gene_data['end']}\"\n",
    "    print(f\"{gene_name}: {genomic_region}\")\n",
    "    return genomic_region\n",
    "\n",
    "kras_genomic_region = get_gene_coordinates_from_ensembl(\"KRAS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1247d2f5",
   "metadata": {},
   "source": [
    "### Somatic Variant Calling (Lofreq)\n",
    "___\n",
    "Replacing `bcftools` with `Lofreq`, a sensitive somatic variant caller optimized for detecting low-frequency variants in ctDNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debcc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_variants_with_lofreq(bam_file_path, reference_fasta_path, genomic_region, output_vcf_path):\n",
    "    \"\"\"\n",
    "    Call somatic variants using Lofreq.\n",
    "    Input: bam_file_path (Path), reference_fasta_path (Path), genomic_region (str), output_vcf_path (Path)\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    # Lofreq requires indel qualities to be inserted into the BAM file\n",
    "    bam_indel_path = bam_file_path.with_suffix(\".indel.bam\")\n",
    "    \n",
    "    if not bam_indel_path.exists():\n",
    "        print(f\"Adding indel qualities to {bam_file_path.name}...\")\n",
    "        run_cmd([\"lofreq\", \"indelqual\", \"--dindel\", \"-f\", str(reference_fasta_path), \"-o\", str(bam_indel_path), str(bam_file_path)])\n",
    "        run_cmd([\"samtools\", \"index\", str(bam_indel_path)])\n",
    "    \n",
    "    if output_vcf_path.exists():\n",
    "        print(f\"Skipping variant calling for {output_vcf_path.name} (already exists)\")\n",
    "        return\n",
    "\n",
    "    print(f\"Calling variants with Lofreq for {bam_file_path.name}...\")\n",
    "    run_cmd([\n",
    "        \"lofreq\", \"call\",\n",
    "        \"-f\", str(reference_fasta_path),\n",
    "        \"-r\", genomic_region,\n",
    "        \"-o\", str(output_vcf_path),\n",
    "        \"--call-indels\",\n",
    "        str(bam_indel_path)\n",
    "    ])\n",
    "\n",
    "vcf_output_directory = data_dir / \"vcf\" / selected_patient_id\n",
    "vcf_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "kras_vcf_file_paths = {}\n",
    "\n",
    "for timepoint_label, bam_file_path in deduplicated_bam_paths.items():\n",
    "    output_vcf_file = vcf_output_directory / f\"{selected_patient_id}_{timepoint_label}_KRAS.lofreq.vcf\"\n",
    "    call_variants_with_lofreq(\n",
    "        bam_file_path=bam_file_path,\n",
    "        reference_fasta_path=fa_ref_genome_path,\n",
    "        genomic_region=kras_genomic_region,\n",
    "        output_vcf_path=output_vcf_file\n",
    "    )\n",
    "    kras_vcf_file_paths[timepoint_label] = output_vcf_file\n",
    "kras_vcf_file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca76c18",
   "metadata": {},
   "source": [
    "### Variant Annotation (SnpEff)\n",
    "___\n",
    "Annotating variants with functional information using SnpEff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed73f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_variants_with_snpeff(input_vcf_path, output_vcf_path, snpeff_db=\"GRCh38.86\"):\n",
    "    \"\"\"\n",
    "    Annotate VCF file using SnpEff, then compress and index with pysam.\n",
    "    Input: input_vcf_path (Path), output_vcf_path (Path), snpeff_db (str)\n",
    "    Output: Path to the compressed and indexed VCF file (.vcf.gz)\n",
    "    \"\"\"\n",
    "    gz_output_path = output_vcf_path.with_suffix(\".vcf.gz\")\n",
    "    \n",
    "    if gz_output_path.exists():\n",
    "        if gz_output_path.stat().st_size > 0:\n",
    "            print(f\"Skipping annotation for {gz_output_path.name} (already exists)\")\n",
    "            return gz_output_path\n",
    "        else:\n",
    "            print(f\"Found empty file {gz_output_path.name}, re-running annotation...\")\n",
    "    \n",
    "    print(f\"Annotating {input_vcf_path.name}...\")\n",
    "    \n",
    "    # Set Java heap size to 10GB to avoid OutOfMemoryError\n",
    "    import os\n",
    "    os.environ[\"_JAVA_OPTIONS\"] = \"-Xmx10g\"\n",
    "    \n",
    "    # Run SnpEff to uncompressed file\n",
    "    with open(output_vcf_path, \"w\") as f:\n",
    "        subprocess.run([\"snpEff\", \"-v\", snpeff_db, str(input_vcf_path)], stdout=f, check=True)\n",
    "    \n",
    "    # Compress and index with pysam\n",
    "    print(f\"Indexing {output_vcf_path.name}...\")\n",
    "    pysam.tabix_index(str(output_vcf_path), preset=\"vcf\", force=True)\n",
    "    \n",
    "    # Clean up uncompressed file if it exists (pysam might keep it or not depending on version/flags, usually keeps input)\n",
    "    if output_vcf_path.exists():\n",
    "        output_vcf_path.unlink()\n",
    "        \n",
    "    return gz_output_path\n",
    "\n",
    "annotated_vcf_paths = {}\n",
    "for timepoint_label, vcf_path in kras_vcf_file_paths.items():\n",
    "    output_ann_vcf = vcf_path.with_name(vcf_path.stem + \".ann.vcf\")\n",
    "    # annotate_variants_with_snpeff returns the .vcf.gz path\n",
    "    annotated_vcf_paths[timepoint_label] = annotate_variants_with_snpeff(vcf_path, output_ann_vcf)\n",
    "annotated_vcf_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ec8fe",
   "metadata": {},
   "source": [
    "### Gene Visualization\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots_sequences import plot_protein_mutations, plot_gene_and_variants, get_protein_features\n",
    "\n",
    "# gff_path is optional. If set to None, the script will automatically fetch gene coordinates from Ensembl.\n",
    "gff_path = None\n",
    "# If you have a local GFF file, you can provide it here:\n",
    "# gff_path = \"data/reference/Homo_sapiens.GRCh38.113.chromosome.12.gff3.gz\"\n",
    "\n",
    "plot_gene_and_variants(\"KRAS\", annotated_vcf_paths, fa_ref_genome_path, gff_path=gff_path, genomic_region=kras_genomic_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Protein Mutations\n",
    "\n",
    "features, length = get_protein_features(\"KRAS\")\n",
    "img = plot_protein_mutations(\"KRAS\", annotated_vcf_paths, protein_features=features, protein_length=length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}